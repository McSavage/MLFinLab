{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a8ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 14:20:02.546317 100.0% apply_pt_sl_on_t1 done after 0.03 minutes. Remaining 0.0 minutes..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of barrier_events: (96, 5)\n",
      "NaN counts in barrier_events:\n",
      "t1      1\n",
      "trgt    0\n",
      "side    0\n",
      "pt      0\n",
      "sl      0\n",
      "dtype: int64\n",
      "\n",
      "First few rows:\n",
      "                                             t1      trgt  side  pt  sl\n",
      "2011-09-25 18:14:00.972 2011-09-26 02:08:27.034  0.014822  -1.0   1   2\n",
      "2011-09-26 02:08:27.034 2011-09-26 21:36:21.122  0.017268  -1.0   1   2\n",
      "2011-09-26 05:43:18.372 2011-09-27 07:20:54.166  0.019288  -1.0   1   2\n",
      "2011-09-26 21:36:21.122 2011-09-28 02:14:52.078  0.020637  -1.0   1   2\n",
      "2011-09-27 05:14:37.147 2011-09-28 08:00:46.833  0.023376  -1.0   1   2\n",
      "\n",
      "Shape after dropping NaN: (95, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 62, 68, 2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlfinlab.util import volatility\n",
    "from mlfinlab.filters import filters\n",
    "from mlfinlab.labeling import labeling\n",
    "from mlfinlab.sampling import bootstrapping\n",
    "from mlfinlab.sampling import concurrent\n",
    "from mlfinlab.data_structures import standard_data_structures\n",
    "\n",
    "sp_read = pd.read_csv('../data/SP.csv')\n",
    "sp = sp_read.loc[pd.to_datetime(sp_read['date'].astype(str)) >= pd.Timestamp('2010-01-01')].copy()\n",
    "sp['date_time'] = pd.to_datetime(sp['date'].astype(str) + ' ' + sp['time'].astype(str))\n",
    "sp['date'] = sp['date_time'].dt.normalize()\n",
    "sp['date_time'] = pd.to_datetime(sp['date'].astype(str) + ' ' + sp['time'].astype(str))\n",
    "sp_processed = sp[['date_time', 'price', 'volume']].copy()\n",
    "sp_processed.columns = ['date_time', 'price', 'volume']\n",
    "\n",
    "data = standard_data_structures.get_dollar_bars(\n",
    "    sp_processed, threshold=1000000, batch_size=100000, verbose=False\n",
    ")\n",
    "\n",
    "data = data.iloc[:2000, :]  # slice the dataset so example doesn't run too long\n",
    "#data.index = pd.to_datetime(data[\"date_time\"])\n",
    "#data = data.drop(\"date_time\", axis=1)\n",
    "# Select the data from 1st September 2011\n",
    "pdata1 = data.copy()\n",
    "\n",
    "data = data[\"2011-09-01\":]\n",
    "# Based on the simple moving average cross-over strategy.\n",
    "# Compute moving averages\n",
    "fast_window = 20\n",
    "slow_window = 50\n",
    "data[\"fast_mavg\"] = (\n",
    "     data[\"close\"]\n",
    "     .rolling(window=fast_window, min_periods=fast_window, center=False)\n",
    "     .mean()\n",
    " )\n",
    "data[\"slow_mavg\"] = (\n",
    "     data[\"close\"]\n",
    "     .rolling(window=slow_window, min_periods=slow_window, center=False)\n",
    "     .mean()\n",
    ")\n",
    "# Compute sides\n",
    "data[\"side\"] = np.nan\n",
    "long_signals = data[\"fast_mavg\"] >= data[\"slow_mavg\"]\n",
    "short_signals = data[\"fast_mavg\"] < data[\"slow_mavg\"]\n",
    "data.loc[long_signals, \"side\"] = 1\n",
    "data.loc[short_signals, \"side\"] = -1\n",
    "# Remove Look ahead biase by lagging the signal\n",
    "data[\"side\"] = data[\"side\"].shift(1)\n",
    "# Duplicate the raw data\n",
    "raw_data = data.copy()\n",
    "# Drop the NaN values from our data set\n",
    "data.dropna(axis=0, how=\"any\", inplace=True)\n",
    "# Compute daily volatility\n",
    "daily_vol = volatility.get_daily_vol(close=data[\"close\"], lookback=50)\n",
    "# Apply Symmetric CUSUM filter and get timestamps for events\n",
    "# Note: Only the CUSUM filter needs a point estimate for volatility\n",
    "cusum_events = filters.cusum_filter(\n",
    "     data[\"close\"], threshold=daily_vol[\"2011-09-01\":\"2018-01-01\"].mean() * 0.5\n",
    ")\n",
    "# Compute (triple barrier labeling) vertical barrier\n",
    "vertical_barriers = labeling.add_vertical_barrier(\n",
    "     t_events=cusum_events, close=data[\"close\"], num_days=1\n",
    ")\n",
    "pt_sl = [1, 2]\n",
    "min_ret = 0.005\n",
    "barrier_events = labeling.get_events(\n",
    "     close=data[\"close\"],\n",
    "     t_events=cusum_events,\n",
    "     pt_sl=pt_sl,\n",
    "     target=daily_vol,\n",
    "     min_ret=min_ret,\n",
    "     num_threads=3,\n",
    "     vertical_barrier_times=vertical_barriers,\n",
    "     side_prediction=data[\"side\"],\n",
    ")\n",
    "barrier_events\n",
    "\n",
    "# Check for NaN values\n",
    "print(f\"Shape of barrier_events: {barrier_events.shape}\")\n",
    "print(f\"NaN counts in barrier_events:\\n{barrier_events.isna().sum()}\")\n",
    "print(f\"\\nFirst few rows:\\n{barrier_events.head()}\")\n",
    "\n",
    "# Remove rows with NaN values\n",
    "barrier_events_clean = barrier_events.dropna()\n",
    "print(f\"\\nShape after dropping NaN: {barrier_events_clean.shape}\")\n",
    "\n",
    "# Use the close prices from dollar bars dataset as the price bars for the indicator matrix.\n",
    "close_prices = pdata1[['close']].copy() \n",
    "\n",
    "\n",
    "# Create the indicator matrix\n",
    "triple_barrier_ind_mat = bootstrapping.get_ind_matrix(barrier_events_clean, close_prices)\n",
    "# MlFinlab can also get average label uniqueness on the indicator matrix\n",
    "ind_mat_uniqueness = bootstrapping.get_ind_mat_average_uniqueness(\n",
    "     triple_barrier_ind_mat\n",
    ")\n",
    "av_unique = concurrent.get_av_uniqueness_from_triple_barrier(\n",
    "     pd.DataFrame(barrier_events_clean), close_prices, num_threads=1\n",
    ")\n",
    "# Draw sequential bootstrap\n",
    "bootstrapping.seq_bootstrap(\n",
    "     triple_barrier_ind_mat, sample_length=4, warmup_samples=[1]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MlFinLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
